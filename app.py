import streamlit as st
import torch
import torch.nn.functional as F
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from model import CNN_LSTM_Model
import os

# --- 1. é…ç½®å‚æ•° ---
INPUT_SIZE = 40
HIDDEN_SIZE = 128
NUM_CLASSES = 6
DURATION = 3
SAMPLE_RATE = 22050
DEVICE = torch.device('cpu')

# æƒ…æ„Ÿæ ‡ç­¾
EMOTION_LABELS = {
    0: 'æ„¤æ€’ (Angry)',
    1: 'ææƒ§ (Fear)',
    2: 'å¿«ä¹ (Happy)',
    3: 'ä¸­æ€§ (Neutral)',
    4: 'æ‚²ä¼¤ (Sad)',
    5: 'æƒŠè®¶ (Surprise)'
}


# --- 2. åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    model = CNN_LSTM_Model(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES)
    model_path = 'best_model.pth'
    if os.path.exists(model_path):
        try:
            model.load_state_dict(torch.load(model_path, map_location=DEVICE))
            model.eval()
            return model
        except Exception as e:
            st.error(f"æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
            return None
    else:
        st.error("âš ï¸ æ‰¾ä¸åˆ° 'best_model.pth'ã€‚è¯·å…ˆè¿è¡Œ train.py è¿›è¡Œè®­ç»ƒï¼")
        return None


# --- 3. é¢„å¤„ç†å‡½æ•° ---
def preprocess_audio(y, sr):
    target_len = SAMPLE_RATE * DURATION
    if len(y) < target_len:
        y = np.pad(y, (0, target_len - len(y)))
    else:
        y = y[:target_len]
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=INPUT_SIZE)
    return mfcc


# --- 4. é›·è¾¾å›¾ç»˜åˆ¶ ---
def plot_radar_chart(probs):
    categories = list(EMOTION_LABELS.values())
    values = list(probs)
    values += [values[0]]
    categories += [categories[0]]

    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill='toself',
        name='æƒ…æ„Ÿæ¦‚ç‡',
        line_color='#FF4B4B',
        fillcolor='rgba(255, 75, 75, 0.3)'
    ))
    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=False,
        title={'text': "ğŸ“Š æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒ", 'y': 0.95, 'x': 0.5, 'xanchor': 'center'},
        margin=dict(l=40, r=40, t=40, b=40)
    )
    return fig


# --- 5. æ ¸å¿ƒåˆ†æé€»è¾‘ ---
def analyze_audio(audio_source):
    # åŠ è½½éŸ³é¢‘ (å…¼å®¹æ–‡ä»¶ä¸Šä¼  å’Œ å½•éŸ³çš„ BytesIO)
    try:
        y, sr = librosa.load(audio_source, sr=SAMPLE_RATE, duration=DURATION)
    except Exception as e:
        st.error(f"éŸ³é¢‘è§£æå¤±è´¥: {e}")
        return

    # éšç§æ¨¡å¼
    if privacy_mode:
        y = librosa.effects.pitch_shift(y, sr=sr, n_steps=4)
        st.toast("å·²åº”ç”¨éšç§è„±æ•å¤„ç†", icon="ğŸ›¡ï¸")

    # å¸ƒå±€
    col1, col2 = st.columns([1, 1.2])

    with col1:
        st.subheader("1. éŸ³é¢‘åˆ†æ")
        st.audio(y, sample_rate=sr)

        fig_wave, ax_wave = plt.subplots(figsize=(6, 2))
        librosa.display.waveshow(y, sr=sr, ax=ax_wave, color='#1f77b4')
        ax_wave.set_title("Waveform")
        st.pyplot(fig_wave)

        st.markdown("**MFCC ç‰¹å¾**")
        mfcc_features = preprocess_audio(y, sr)
        fig_spec, ax_spec = plt.subplots(figsize=(6, 2))
        img = librosa.display.specshow(mfcc_features, x_axis='time', ax=ax_spec, cmap='inferno')
        fig_spec.colorbar(img, ax=ax_spec, format="%+2.f dB")
        st.pyplot(fig_spec)

    with col2:
        st.subheader("2. è¯†åˆ«ç»“æœ")
        input_tensor = torch.tensor(mfcc_features, dtype=torch.float32).unsqueeze(0)

        model = load_model()
        if model:
            with torch.no_grad():
                logits = model(input_tensor)
                probs = F.softmax(logits, dim=1).numpy()[0]
                pred_idx = np.argmax(probs)

            top_emotion = EMOTION_LABELS[pred_idx]
            confidence = probs[pred_idx] * 100

            if confidence > 70:
                st.success(f"### ğŸ¯ è¯†åˆ«æƒ…æ„Ÿï¼š{top_emotion}")
            elif confidence > 40:
                st.warning(f"### âš ï¸ è¯†åˆ«æƒ…æ„Ÿï¼š{top_emotion}")
            else:
                st.error(f"### â“ è¯†åˆ«æƒ…æ„Ÿï¼š{top_emotion}")

            st.write(f"**ç½®ä¿¡åº¦:** {confidence:.2f}%")
            st.plotly_chart(plot_radar_chart(probs), use_container_width=True)


# --- 6. é¡µé¢ä¸»å…¥å£ ---
st.set_page_config(page_title="è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ", layout="wide", page_icon="ğŸ™ï¸")

st.title("ğŸ™ï¸ åŸºäº CNN-LSTM-Attention çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ")
st.markdown("### ğŸ“ æœ¬ç§‘æ¯•ä¸šè®¾è®¡æ¼”ç¤º | é‚µé‡‘æ¡¥")
st.write("---")

with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    privacy_mode = st.checkbox("ğŸ›¡ï¸ å¼€å¯éšç§ä¿æŠ¤æ¨¡å¼", value=False)
    st.info("ğŸ’¡ **è¯´æ˜**ï¼š\nå¯é€‰æ‹©ä¸Šä¼ æ–‡ä»¶æˆ–ç›´æ¥ä½¿ç”¨éº¦å…‹é£å½•éŸ³ã€‚")

tab1, tab2 = st.tabs(["ğŸ“‚ ä¸Šä¼ æ–‡ä»¶æ¨¡å¼", "ğŸ¤ å®æ—¶å½•éŸ³æ¨¡å¼"])

# --- Tab 1: ä¸Šä¼ æ–‡ä»¶ ---
with tab1:
    uploaded_file = st.file_uploader("è¯·ä¸Šä¼ ä¸€æ®µè¯­éŸ³æ–‡ä»¶ (.wav)", type=['wav'])
    if uploaded_file is not None:
        analyze_audio(uploaded_file)

# --- Tab 2: å®æ—¶å½•éŸ³ (ä½¿ç”¨å®˜æ–¹åŸç”Ÿç»„ä»¶) ---
with tab2:
    st.write("ç‚¹å‡»ä¸‹æ–¹çº¢è‰²æŒ‰é’®å¼€å§‹å½•éŸ³ï¼š")
    # âœ¨âœ¨âœ¨ é‡ç‚¹ï¼šç›´æ¥ä½¿ç”¨ st.audio_inputï¼Œä¸éœ€è¦å®‰è£…ä»»ä½•åº“ï¼ âœ¨âœ¨âœ¨
    audio_value = st.audio_input("æŒ‰ä½å½•éŸ³")

    if audio_value:
        st.success("âœ… å½•éŸ³å®Œæˆï¼Œæ­£åœ¨åˆ†æ...")
        analyze_audio(audio_value)
