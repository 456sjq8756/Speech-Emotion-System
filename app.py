import streamlit as st
import torch
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
from model import CNN_LSTM_Model
import os

# --- 1. é…ç½®å‚æ•° (å¿…é¡»ä¸ train.py ä¸€è‡´) ---
INPUT_SIZE = 40
HIDDEN_SIZE = 128
NUM_CLASSES = 6
DURATION = 3
SAMPLE_RATE = 22050
DEVICE = torch.device('cpu')  # æ¨ç†æ—¶ç”¨ CPU å°±å¤Ÿäº†

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„ (æ ¹æ®ä½  make_data.py é‡Œçš„é¡ºåºï¼Œåå‘æ˜ å°„)
# å‡è®¾é¡ºåºæ˜¯: 0:angry, 1:fear, 2:happy, 3:neutral, 4:sad, 5:surprise
# è¯·æ ¹æ®ä½ å®é™…è®­ç»ƒæ—¶çš„ log è¾“å‡ºæ ¸å¯¹ä¸€ä¸‹ï¼Œå¦‚æœä¸ç¡®å®šï¼Œå…ˆç”¨è¿™ä¸ªè¯•è¯•
EMOTION_LABELS = {
    0: 'æ„¤æ€’ (Angry)',
    1: 'ææƒ§ (Fear)',
    2: 'å¿«ä¹ (Happy)',
    3: 'ä¸­æ€§ (Neutral)',
    4: 'æ‚²ä¼¤ (Sad)',
    5: 'æƒŠè®¶ (Surprise)'
}


# --- 2. åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    model = CNN_LSTM_Model(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES)
    # åŠ è½½ä½ åˆšæ‰è®­ç»ƒå¥½çš„æƒé‡
    if os.path.exists('best_model.pth'):
        model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))
    else:
        st.error("æ‰¾ä¸åˆ° best_model.pthï¼Œè¯·å…ˆè¿è¡Œ train.pyï¼")
    model.eval()
    return model


# --- 3. é¢„å¤„ç†å‡½æ•° (é€»è¾‘å¿…é¡»ä¸ make_data.py ä¸€è‡´) ---
def preprocess_audio(y, sr):
    # ç»Ÿä¸€é•¿åº¦
    target_len = SAMPLE_RATE * DURATION
    if len(y) < target_len:
        y = np.pad(y, (0, target_len - len(y)))
    else:
        y = y[:target_len]

    # æå– MFCC
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=INPUT_SIZE)
    # è½¬ç½®: (Features, Time) -> (Time, Features)
    mfcc = mfcc.T
    return mfcc


# --- 4. é¡µé¢å¸ƒå±€ ---
st.set_page_config(page_title="è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ", layout="wide")

st.title("ğŸ™ï¸ åŸºäº CNN-LSTM çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ")
st.markdown("### ğŸ“ æœ¬ç§‘æ¯•ä¸šè®¾è®¡æ¼”ç¤º | é‚µé‡‘æ¡¥")
st.write("---")

# ä¾§è¾¹æ 
st.sidebar.header("åŠŸèƒ½æ§åˆ¶")
privacy_mode = st.sidebar.checkbox("ğŸ›¡ï¸ å¼€å¯éšç§ä¿æŠ¤æ¨¡å¼ (å˜å£°è„±æ•)", value=False)
st.sidebar.info("è¯´æ˜ï¼šå¼€å¯éšç§æ¨¡å¼åï¼Œç³»ç»Ÿå°†å¯¹éŸ³é¢‘è¿›è¡Œå˜è°ƒå¤„ç†ï¼Œä¿æŠ¤è¯´è¯äººéŸ³è‰²ï¼Œä½†æ¨¡å‹ä»èƒ½è¯†åˆ«æƒ…æ„Ÿã€‚")

# ä¸»åŒºåŸŸ
col1, col2 = st.columns([1, 1])

uploaded_file = st.file_uploader("ğŸ“‚ è¯·ä¸Šä¼ ä¸€æ®µè¯­éŸ³æ–‡ä»¶ (.wav)", type=['wav'])

if uploaded_file is not None:
    # 1. åŠ è½½éŸ³é¢‘
    y, sr = librosa.load(uploaded_file, sr=SAMPLE_RATE, duration=DURATION)

    # éšç§ä¿æŠ¤å¤„ç† (å˜å£°)
    if privacy_mode:
        y = librosa.effects.pitch_shift(y, sr=sr, n_steps=4)  # å‡é«˜4ä¸ªåŠéŸ³
        st.toast("å·²åº”ç”¨éšç§è„±æ•å¤„ç†", icon="ğŸ›¡ï¸")

    # 2. æ’­æ”¾éŸ³é¢‘
    with col1:
        st.subheader("1. éŸ³é¢‘æ’­æ”¾ & æ³¢å½¢")
        # âœ… ä¿®æ­£åï¼šç›´æ¥æ’­æ”¾å¤„ç†åçš„ä¿¡å· y
        # sample_rate å¿…é¡»æŒ‡å®šï¼Œå¦åˆ™æ’­æ”¾é€Ÿåº¦ä¼šä¸å¯¹
        st.audio(y, sample_rate=sr)

        # ç»˜åˆ¶æ³¢å½¢å›¾
        fig_wave, ax_wave = plt.subplots(figsize=(6, 2))
        librosa.display.waveshow(y, sr=sr, ax=ax_wave, color='blue')
        ax_wave.set_title("Waveform")
        st.pyplot(fig_wave)

    # 3. æå–ç‰¹å¾å¹¶æ¨ç†
    mfcc_features = preprocess_audio(y, sr)

    # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼: (1, Channels, Time) -> æ³¨æ„è¿™é‡Œè¿˜éœ€è¦ transpose
    # make_data é‡Œçš„ Dataset åšäº†ä¸€æ¬¡ transpose(0,1)ï¼Œæ‰€ä»¥è¿™é‡Œä¹Ÿè¦å¯¹é½
    # ç‰¹å¾ shape: (300, 40)
    input_tensor = torch.tensor(mfcc_features, dtype=torch.float32)  # (Time, Feat)
    input_tensor = input_tensor.transpose(0, 1)  # (Feat, Time) -> (40, 300)
    input_tensor = input_tensor.unsqueeze(0)  # (Batch, Feat, Time) -> (1, 40, 300)

    # æ¨¡å‹æ¨ç†
    model = load_model()
    with torch.no_grad():
        logits = model(input_tensor)
        probs = torch.softmax(logits, dim=1).numpy()[0]
        pred_label = np.argmax(probs)

    # 4. å±•ç¤ºç»“æœ
    with col2:
        st.subheader("2. è¯†åˆ«ç»“æœ")

        # ç»“æœå¤§å­—å±•ç¤º
        emotion_name = EMOTION_LABELS.get(pred_label, "æœªçŸ¥")
        confidence = probs[pred_label] * 100

        if confidence > 60:
            st.success(f"è¯†åˆ«æƒ…æ„Ÿï¼š**{emotion_name}** (ç½®ä¿¡åº¦: {confidence:.1f}%)")
        else:
            st.warning(f"è¯†åˆ«æƒ…æ„Ÿï¼š**{emotion_name}** (ç½®ä¿¡åº¦è¾ƒä½: {confidence:.1f}%)")

        # æ¦‚ç‡åˆ†å¸ƒæŸ±çŠ¶å›¾
        st.write("å„æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒï¼š")
        chart_data = {label: prob for label, prob in zip(EMOTION_LABELS.values(), probs)}
        st.bar_chart(chart_data)

    # 5. ç‰¹å¾å¯è§†åŒ– (å£°è°±å›¾)
    st.write("---")
    st.subheader("3. æ·±åº¦ç‰¹å¾å¯è§†åŒ– (MFCC çƒ­åŠ›å›¾)")
    fig_spec, ax_spec = plt.subplots(figsize=(10, 3))
    img = librosa.display.specshow(mfcc_features.T, x_axis='time', ax=ax_spec, cmap='viridis')
    fig_spec.colorbar(img, ax=ax_spec, format="%+2.f dB")
    ax_spec.set_title("MFCC Spectrogram")
    st.pyplot(fig_spec)
