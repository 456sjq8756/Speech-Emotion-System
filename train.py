import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from dataset import load_data
from model import CNN_LSTM_Model
import os

# --- 1. è¶…å‚æ•°é…ç½® ---
BATCH_SIZE = 16  # æ¯ä¸€æ‰¹è®­ç»ƒå¤šå°‘ä¸ªæ ·æœ¬ (ç”µè„‘å¡å°±æ”¹å°ç‚¹ï¼Œæ¯”å¦‚ 8)
LEARNING_RATE = 0.001  # å­¦ä¹ ç‡
EPOCHS = 50  # è®­ç»ƒè½®æ•° (å…ˆè·‘50è½®è¯•è¯•)
NUM_CLASSES = 6  # å¯¹åº” make_data.py é‡Œçš„ 6 ç§æƒ…æ„Ÿ
INPUT_SIZE = 40  # MFCC ç‰¹å¾ç»´åº¦

# æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾å¡ (ä½ ä¹‹å‰æµ‹è¯•æ˜¯ CPUï¼Œè¿™é‡Œä¼šè‡ªåŠ¨é€‚é…)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"å½“å‰ä½¿ç”¨çš„è®­ç»ƒè®¾å¤‡: {DEVICE}")


def train():
    # --- 2. åŠ è½½æ•°æ® ---
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    train_dataset, test_dataset = load_data()

    if train_dataset is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ make_data.py æ˜¯å¦è¿è¡ŒæˆåŠŸ")
        return

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"è®­ç»ƒé›†æ•°é‡: {len(train_dataset)}, æµ‹è¯•é›†æ•°é‡: {len(test_dataset)}")

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ ---
    model = CNN_LSTM_Model(input_size=INPUT_SIZE, hidden_size=128, num_classes=NUM_CLASSES)
    model = model.to(DEVICE)  # æ¬åˆ° GPU/CPU

    # --- 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # è®°å½•æœ€ä½³å‡†ç¡®ç‡
    best_acc = 0.0

    # --- 5. å¼€å§‹è®­ç»ƒå¾ªç¯ ---
    for epoch in range(EPOCHS):
        model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        running_loss = 0.0

        for features, labels in train_loader:
            features, labels = features.to(DEVICE), labels.to(DEVICE)

            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(features)

            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # --- 6. æ¯ä¸ª Epoch ç»“æŸåè¿›è¡Œæµ‹è¯• (éªŒè¯) ---
        model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        correct = 0
        total = 0
        with torch.no_grad():  # æµ‹è¯•æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œçœå†…å­˜
            for features, labels in test_loader:
                features, labels = features.to(DEVICE), labels.to(DEVICE)
                outputs = model(features)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        epoch_acc = 100 * correct / total
        avg_loss = running_loss / len(train_loader)

        print(f"Epoch [{epoch + 1}/{EPOCHS}] -> Loss: {avg_loss:.4f} | Accuracy: {epoch_acc:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"  ğŸ”¥ å‡†ç¡®ç‡æå‡ï¼æ¨¡å‹å·²ä¿å­˜ä¸º best_model.pth")

    print("\nè®­ç»ƒç»“æŸï¼")
    print(f"æœ€é«˜å‡†ç¡®ç‡: {best_acc:.2f}%")
    print("è¯·ä½¿ç”¨ 'best_model.pth' è¿›è¡Œåç»­çš„å¯è§†åŒ–å±•ç¤ºã€‚")


if __name__ == '__main__':
    train()
